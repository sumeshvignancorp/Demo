<html>
<head>
<title>METADATA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
METADATA</font>
</center></td></tr></table>
<pre><span class="s0">Metadata-Version: 2.1</span>
<span class="s0">Name: charset-normalizer</span>
<span class="s0">Version: 3.2.0</span>
<span class="s0">Summary: The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.</span>
<span class="s0">Home-page: https://github.com/Ousret/charset_normalizer</span>
<span class="s0">Author: Ahmed TAHRI</span>
<span class="s0">Author-email: ahmed.tahri@cloudnursery.dev</span>
<span class="s0">License: MIT</span>
<span class="s0">Project-URL: Bug Reports, https://github.com/Ousret/charset_normalizer/issues</span>
<span class="s0">Project-URL: Documentation, https://charset-normalizer.readthedocs.io/en/latest</span>
<span class="s0">Keywords: encoding,charset,charset-detector,detector,normalization,unicode,chardet,detect</span>
<span class="s0">Classifier: Development Status :: 5 - Production/Stable</span>
<span class="s0">Classifier: License :: OSI Approved :: MIT License</span>
<span class="s0">Classifier: Intended Audience :: Developers</span>
<span class="s0">Classifier: Topic :: Software Development :: Libraries :: Python Modules</span>
<span class="s0">Classifier: Operating System :: OS Independent</span>
<span class="s0">Classifier: Programming Language :: Python</span>
<span class="s0">Classifier: Programming Language :: Python :: 3</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.7</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.8</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.9</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.10</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.11</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.12</span>
<span class="s0">Classifier: Programming Language :: Python :: Implementation :: PyPy</span>
<span class="s0">Classifier: Topic :: Text Processing :: Linguistic</span>
<span class="s0">Classifier: Topic :: Utilities</span>
<span class="s0">Classifier: Typing :: Typed</span>
<span class="s0">Requires-Python: &gt;=3.7.0</span>
<span class="s0">Description-Content-Type: text/markdown</span>
<span class="s0">License-File: LICENSE</span>
<span class="s0">Provides-Extra: unicode_backport</span>

<span class="s0">&lt;h1 align=&quot;center&quot;&gt;Charset Detection, for Everyone üëã&lt;/h1&gt;</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
  <span class="s0">&lt;sup&gt;The Real First Universal Charset Detector&lt;/sup&gt;&lt;br&gt;</span>
  <span class="s0">&lt;a href=&quot;https://pypi.org/project/charset-normalizer&quot;&gt;</span>
    <span class="s0">&lt;img src=&quot;https://img.shields.io/pypi/pyversions/charset_normalizer.svg?orange=blue&quot; /&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
  <span class="s0">&lt;a href=&quot;https://pepy.tech/project/charset-normalizer/&quot;&gt;</span>
    <span class="s0">&lt;img alt=&quot;Download Count Total&quot; src=&quot;https://pepy.tech/badge/charset-normalizer/month&quot; /&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
  <span class="s0">&lt;a href=&quot;https://bestpractices.coreinfrastructure.org/projects/7297&quot;&gt;</span>
    <span class="s0">&lt;img src=&quot;https://bestpractices.coreinfrastructure.org/projects/7297/badge&quot;&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
<span class="s0">&lt;/p&gt;</span>

<span class="s0">&gt; A library that helps you read text from an unknown charset encoding.&lt;br /&gt; Motivated by `chardet`,</span>
<span class="s0">&gt; I'm trying to resolve the issue by taking a new approach.</span>
<span class="s0">&gt; All IANA character set names for which the Python core library provides codecs are supported.</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
  <span class="s0">&gt;&gt;&gt;&gt;&gt; &lt;a href=&quot;https://charsetnormalizerweb.ousret.now.sh&quot; target=&quot;_blank&quot;&gt;üëâ Try Me Online Now, Then Adopt Me üëà &lt;/a&gt; &lt;&lt;&lt;&lt;&lt;</span>
<span class="s0">&lt;/p&gt;</span>

<span class="s0">This project offers you an alternative to **Universal Charset Encoding Detector**, also known as **Chardet**.</span>

<span class="s0">| Feature                                          | [Chardet](https://github.com/chardet/chardet) |                                           Charset Normalizer                                           | [cChardet](https://github.com/PyYoshi/cChardet) |</span>
<span class="s0">|--------------------------------------------------|:---------------------------------------------:|:------------------------------------------------------------------------------------------------------:|:-----------------------------------------------:|</span>
<span class="s0">| `Fast`                                           |                     ‚ùå&lt;br&gt;                     |                                                 ‚úÖ&lt;br&gt;                                                  |                     ‚úÖ &lt;br&gt;                      |</span>
<span class="s0">| `Universal**`                                    |                       ‚ùå                       |                                                   ‚úÖ                                                    |                        ‚ùå                        |</span>
<span class="s0">| `Reliable` **without** distinguishable standards |                       ‚ùå                       |                                                   ‚úÖ                                                    |                        ‚úÖ                        |</span>
<span class="s0">| `Reliable` **with** distinguishable standards    |                       ‚úÖ                       |                                                   ‚úÖ                                                    |                        ‚úÖ                        |</span>
<span class="s0">| `License`                                        |           LGPL-2.1&lt;br&gt;_restrictive_           |                                                  MIT                                                   |            MPL-1.1&lt;br&gt;_restrictive_             |</span>
<span class="s0">| `Native Python`                                  |                       ‚úÖ                       |                                                   ‚úÖ                                                    |                        ‚ùå                        |</span>
<span class="s0">| `Detect spoken language`                         |                       ‚ùå                       |                                                   ‚úÖ                                                    |                       N/A                       |</span>
<span class="s0">| `UnicodeDecodeError Safety`                      |                       ‚ùå                       |                                                   ‚úÖ                                                    |                        ‚ùå                        |</span>
<span class="s0">| `Whl Size`                                       |                   193.6 kB                    |                                                 40 kB                                                  |                     ~200 kB                     |</span>
<span class="s0">| `Supported Encoding`                             |                      33                       | üéâ [90](https://charset-normalizer.readthedocs.io/en/latest/user/support.html#supported-encodings) |                       40                        |</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
<span class="s0">&lt;img src=&quot;https://i.imgflip.com/373iay.gif&quot; alt=&quot;Reading Normalized Text&quot; width=&quot;226&quot;/&gt;&lt;img src=&quot;https://media.tenor.com/images/c0180f70732a18b4965448d33adba3d0/tenor.gif&quot; alt=&quot;Cat Reading Text&quot; width=&quot;200&quot;/&gt;</span>

<span class="s0">*\*\* : They are clearly using specific code for a specific encoding even if covering most of used one*&lt;br&gt; </span>
<span class="s0">Did you got there because of the logs? See [https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html](https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html)</span>

<span class="s0">## ‚ö° Performance</span>

<span class="s0">This package offer better performance than its counterpart Chardet. Here are some numbers.</span>

<span class="s0">| Package                                       | Accuracy | Mean per file (ms) | File per sec (est) |</span>
<span class="s0">|-----------------------------------------------|:--------:|:------------------:|:------------------:|</span>
<span class="s0">| [chardet](https://github.com/chardet/chardet) |   86 %   |       200 ms       |     5 file/sec     |</span>
<span class="s0">| charset-normalizer                            | **98 %** |     **10 ms**      |    100 file/sec    |</span>

<span class="s0">| Package                                       | 99th percentile | 95th percentile | 50th percentile |</span>
<span class="s0">|-----------------------------------------------|:---------------:|:---------------:|:---------------:|</span>
<span class="s0">| [chardet](https://github.com/chardet/chardet) |     1200 ms     |     287 ms      |      23 ms      |</span>
<span class="s0">| charset-normalizer                            |     100 ms      |      50 ms      |      5 ms       |</span>

<span class="s0">Chardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.</span>

<span class="s0">&gt; Stats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.</span>
<span class="s0">&gt; And yes, these results might change at any time. The dataset can be updated to include more files.</span>
<span class="s0">&gt; The actual delays heavily depends on your CPU capabilities. The factors should remain the same.</span>
<span class="s0">&gt; Keep in mind that the stats are generous and that Chardet accuracy vs our is measured using Chardet initial capability</span>
<span class="s0">&gt; (eg. Supported Encoding) Challenge-them if you want.</span>

<span class="s0">## ‚ú® Installation</span>

<span class="s0">Using pip:</span>

<span class="s0">```sh</span>
<span class="s0">pip install charset-normalizer -U</span>
<span class="s0">```</span>

<span class="s0">## üöÄ Basic Usage</span>

<span class="s0">### CLI</span>
<span class="s0">This package comes with a CLI.</span>

<span class="s0">```</span>
<span class="s0">usage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]</span>
                  <span class="s0">file [file ...]</span>

<span class="s0">The Real First Universal Charset Detector. Discover originating encoding used</span>
<span class="s0">on text file. Normalize text to unicode.</span>

<span class="s0">positional arguments:</span>
  <span class="s0">files                 File(s) to be analysed</span>

<span class="s0">optional arguments:</span>
  <span class="s0">-h, --help            show this help message and exit</span>
  <span class="s0">-v, --verbose         Display complementary information about file if any.</span>
                        <span class="s0">Stdout will contain logs about the detection process.</span>
  <span class="s0">-a, --with-alternative</span>
                        <span class="s0">Output complementary possibilities if any. Top-level</span>
                        <span class="s0">JSON WILL be a list.</span>
  <span class="s0">-n, --normalize       Permit to normalize input file. If not set, program</span>
                        <span class="s0">does not write anything.</span>
  <span class="s0">-m, --minimal         Only output the charset detected to STDOUT. Disabling</span>
                        <span class="s0">JSON output.</span>
  <span class="s0">-r, --replace         Replace file when trying to normalize it instead of</span>
                        <span class="s0">creating a new one.</span>
  <span class="s0">-f, --force           Replace file without asking if you are sure, use this</span>
                        <span class="s0">flag with caution.</span>
  <span class="s0">-t THRESHOLD, --threshold THRESHOLD</span>
                        <span class="s0">Define a custom maximum amount of chaos allowed in</span>
                        <span class="s0">decoded content. 0. &lt;= chaos &lt;= 1.</span>
  <span class="s0">--version             Show version information and exit.</span>
<span class="s0">```</span>

<span class="s0">```bash</span>
<span class="s0">normalizer ./data/sample.1.fr.srt</span>
<span class="s0">```</span>

<span class="s0">üéâ Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.</span>

<span class="s0">```json</span>
<span class="s0">{</span>
    <span class="s0">&quot;path&quot;: &quot;/home/default/projects/charset_normalizer/data/sample.1.fr.srt&quot;,</span>
    <span class="s0">&quot;encoding&quot;: &quot;cp1252&quot;,</span>
    <span class="s0">&quot;encoding_aliases&quot;: [</span>
        <span class="s0">&quot;1252&quot;,</span>
        <span class="s0">&quot;windows_1252&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;alternative_encodings&quot;: [</span>
        <span class="s0">&quot;cp1254&quot;,</span>
        <span class="s0">&quot;cp1256&quot;,</span>
        <span class="s0">&quot;cp1258&quot;,</span>
        <span class="s0">&quot;iso8859_14&quot;,</span>
        <span class="s0">&quot;iso8859_15&quot;,</span>
        <span class="s0">&quot;iso8859_16&quot;,</span>
        <span class="s0">&quot;iso8859_3&quot;,</span>
        <span class="s0">&quot;iso8859_9&quot;,</span>
        <span class="s0">&quot;latin_1&quot;,</span>
        <span class="s0">&quot;mbcs&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;language&quot;: &quot;French&quot;,</span>
    <span class="s0">&quot;alphabets&quot;: [</span>
        <span class="s0">&quot;Basic Latin&quot;,</span>
        <span class="s0">&quot;Latin-1 Supplement&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;has_sig_or_bom&quot;: false,</span>
    <span class="s0">&quot;chaos&quot;: 0.149,</span>
    <span class="s0">&quot;coherence&quot;: 97.152,</span>
    <span class="s0">&quot;unicode_path&quot;: null,</span>
    <span class="s0">&quot;is_preferred&quot;: true</span>
<span class="s0">}</span>
<span class="s0">```</span>

<span class="s0">### Python</span>
<span class="s0">*Just print out normalized text*</span>
<span class="s0">```python</span>
<span class="s0">from charset_normalizer import from_path</span>

<span class="s0">results = from_path('./my_subtitle.srt')</span>

<span class="s0">print(str(results.best()))</span>
<span class="s0">```</span>

<span class="s0">*Upgrade your code without effort*</span>
<span class="s0">```python</span>
<span class="s0">from charset_normalizer import detect</span>
<span class="s0">```</span>

<span class="s0">The above code will behave the same as **chardet**. We ensure that we offer the best (reasonable) BC result possible.</span>

<span class="s0">See the docs for advanced usage : [readthedocs.io](https://charset-normalizer.readthedocs.io/en/latest/)</span>

<span class="s0">## üòá Why</span>

<span class="s0">When I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a</span>
<span class="s0">reliable alternative using a completely different method. Also! I never back down on a good challenge!</span>

<span class="s0">I **don't care** about the **originating charset** encoding, because **two different tables** can</span>
<span class="s0">produce **two identical rendered string.**</span>
<span class="s0">What I want is to get readable text, the best I can. </span>

<span class="s0">In a way, **I'm brute forcing text decoding.** How cool is that ? üòé</span>

<span class="s0">Don't confuse package **ftfy** with charset-normalizer or chardet. ftfy goal is to repair unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.</span>

<span class="s0">## üç∞ How</span>

  <span class="s0">- Discard all charset encoding table that could not fit the binary content.</span>
  <span class="s0">- Measure noise, or the mess once opened (by chunks) with a corresponding charset encoding.</span>
  <span class="s0">- Extract matches with the lowest mess detected.</span>
  <span class="s0">- Additionally, we measure coherence / probe for a language.</span>

<span class="s0">**Wait a minute**, what is noise/mess and coherence according to **YOU ?**</span>

<span class="s0">*Noise :* I opened hundred of text files, **written by humans**, with the wrong encoding table. **I observed**, then</span>
<span class="s0">**I established** some ground rules about **what is obvious** when **it seems like** a mess.</span>
 <span class="s0">I know that my interpretation of what is noise is probably incomplete, feel free to contribute in order to</span>
 <span class="s0">improve or rewrite it.</span>

<span class="s0">*Coherence :* For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought</span>
<span class="s0">that intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.</span>

<span class="s0">## ‚ö° Known limitations</span>

  <span class="s0">- Language detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))</span>
  <span class="s0">- Every charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.</span>

<span class="s0">## ‚ö†Ô∏è About Python EOLs</span>

<span class="s0">**If you are running:**</span>

<span class="s0">- Python &gt;=2.7,&lt;3.5: Unsupported</span>
<span class="s0">- Python 3.5: charset-normalizer &lt; 2.1</span>
<span class="s0">- Python 3.6: charset-normalizer &lt; 3.1</span>

<span class="s0">Upgrade your Python interpreter as soon as possible.</span>

<span class="s0">## üë§ Contributing</span>

<span class="s0">Contributions, issues and feature requests are very much welcome.&lt;br /&gt;</span>
<span class="s0">Feel free to check [issues page](https://github.com/ousret/charset_normalizer/issues) if you want to contribute.</span>

<span class="s0">## üìù License</span>

<span class="s0">Copyright ¬© [Ahmed TAHRI @Ousret](https://github.com/Ousret).&lt;br /&gt;</span>
<span class="s0">This project is [MIT](https://github.com/Ousret/charset_normalizer/blob/master/LICENSE) licensed.</span>

<span class="s0">Characters frequencies used in this project ¬© 2012 [Denny Vrandeƒçiƒá](http://simia.net/letters/)</span>

<span class="s0">## üíº For Enterprise</span>

<span class="s0">Professional support for charset-normalizer is available as part of the [Tidelift</span>
<span class="s0">Subscription][1]. Tidelift gives software development teams a single source for</span>
<span class="s0">purchasing and maintaining their software, with professional grade assurances</span>
<span class="s0">from the experts who know it best, while seamlessly integrating with existing</span>
<span class="s0">tools.</span>

<span class="s0">[1]: https://tidelift.com/subscription/pkg/pypi-charset-normalizer?utm_source=pypi-charset-normalizer&amp;utm_medium=readme</span>

<span class="s0"># Changelog</span>
<span class="s0">All notable changes to charset-normalizer will be documented in this file. This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).</span>
<span class="s0">The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).</span>

<span class="s0">## [3.2.0](https://github.com/Ousret/charset_normalizer/compare/3.1.0...3.2.0) (2023-06-07)</span>

<span class="s0">### Changed</span>
<span class="s0">- Typehint for function `from_path` no longer enforce `PathLike` as its first argument</span>
<span class="s0">- Minor improvement over the global detection reliability</span>

<span class="s0">### Added</span>
<span class="s0">- Introduce function `is_binary` that relies on main capabilities, and optimized to detect binaries</span>
<span class="s0">- Propagate `enable_fallback` argument throughout `from_bytes`, `from_path`, and `from_fp` that allow a deeper control over the detection (default True)</span>
<span class="s0">- Explicit support for Python 3.12</span>

<span class="s0">### Fixed</span>
<span class="s0">- Edge case detection failure where a file would contain 'very-long' camel cased word (Issue #289)</span>

<span class="s0">## [3.1.0](https://github.com/Ousret/charset_normalizer/compare/3.0.1...3.1.0) (2023-03-06)</span>

<span class="s0">### Added</span>
<span class="s0">- Argument `should_rename_legacy` for legacy function `detect` and disregard any new arguments without errors (PR #262)</span>

<span class="s0">### Removed</span>
<span class="s0">- Support for Python 3.6 (PR #260)</span>

<span class="s0">### Changed</span>
<span class="s0">- Optional speedup provided by mypy/c 1.0.1</span>

<span class="s0">## [3.0.1](https://github.com/Ousret/charset_normalizer/compare/3.0.0...3.0.1) (2022-11-18)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Multi-bytes cutter/chunk generator did not always cut correctly (PR #233)</span>

<span class="s0">### Changed</span>
<span class="s0">- Speedup provided by mypy/c 0.990 on Python &gt;= 3.7</span>

<span class="s0">## [3.0.0](https://github.com/Ousret/charset_normalizer/compare/2.1.1...3.0.0) (2022-10-20)</span>

<span class="s0">### Added</span>
<span class="s0">- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results</span>
<span class="s0">- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES</span>
<span class="s0">- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio</span>
<span class="s0">- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)</span>

<span class="s0">### Changed</span>
<span class="s0">- Build with static metadata using 'build' frontend</span>
<span class="s0">- Make the language detection stricter</span>
<span class="s0">- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1</span>

<span class="s0">### Fixed</span>
<span class="s0">- CLI with opt --normalize fail when using full path for files</span>
<span class="s0">- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it</span>
<span class="s0">- Sphinx warnings when generating the documentation</span>

<span class="s0">### Removed</span>
<span class="s0">- Coherence detector no longer return 'Simple English' instead return 'English'</span>
<span class="s0">- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'</span>
<span class="s0">- Breaking: Method `first()` and `best()` from CharsetMatch</span>
<span class="s0">- UTF-7 will no longer appear as &quot;detected&quot; without a recognized SIG/mark (is unreliable/conflict with ASCII)</span>
<span class="s0">- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches</span>
<span class="s0">- Breaking: Top-level function `normalize`</span>
<span class="s0">- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch</span>
<span class="s0">- Support for the backport `unicodedata2`</span>

<span class="s0">## [3.0.0rc1](https://github.com/Ousret/charset_normalizer/compare/3.0.0b2...3.0.0rc1) (2022-10-18)</span>

<span class="s0">### Added</span>
<span class="s0">- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results</span>
<span class="s0">- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES</span>
<span class="s0">- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio</span>

<span class="s0">### Changed</span>
<span class="s0">- Build with static metadata using 'build' frontend</span>
<span class="s0">- Make the language detection stricter</span>

<span class="s0">### Fixed</span>
<span class="s0">- CLI with opt --normalize fail when using full path for files</span>
<span class="s0">- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it</span>

<span class="s0">### Removed</span>
<span class="s0">- Coherence detector no longer return 'Simple English' instead return 'English'</span>
<span class="s0">- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'</span>

<span class="s0">## [3.0.0b2](https://github.com/Ousret/charset_normalizer/compare/3.0.0b1...3.0.0b2) (2022-08-21)</span>

<span class="s0">### Added</span>
<span class="s0">- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)</span>

<span class="s0">### Removed</span>
<span class="s0">- Breaking: Method `first()` and `best()` from CharsetMatch</span>
<span class="s0">- UTF-7 will no longer appear as &quot;detected&quot; without a recognized SIG/mark (is unreliable/conflict with ASCII)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Sphinx warnings when generating the documentation</span>

<span class="s0">## [3.0.0b1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...3.0.0b1) (2022-08-15)</span>

<span class="s0">### Changed</span>
<span class="s0">- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1</span>

<span class="s0">### Removed</span>
<span class="s0">- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches</span>
<span class="s0">- Breaking: Top-level function `normalize`</span>
<span class="s0">- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch</span>
<span class="s0">- Support for the backport `unicodedata2`</span>

<span class="s0">## [2.1.1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...2.1.1) (2022-08-19)</span>

<span class="s0">### Deprecated</span>
<span class="s0">- Function `normalize` scheduled for removal in 3.0</span>

<span class="s0">### Changed</span>
<span class="s0">- Removed useless call to decode in fn is_unprintable (#206)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Third-party library (i18n xgettext) crashing not recognizing utf_8 (PEP 263) with underscore from [@aleksandernovikov](https://github.com/aleksandernovikov) (#204)</span>

<span class="s0">## [2.1.0](https://github.com/Ousret/charset_normalizer/compare/2.0.12...2.1.0) (2022-06-19)</span>

<span class="s0">### Added</span>
<span class="s0">- Output the Unicode table version when running the CLI with `--version` (PR #194)</span>

<span class="s0">### Changed</span>
<span class="s0">- Re-use decoded buffer for single byte character sets from [@nijel](https://github.com/nijel) (PR #175)</span>
<span class="s0">- Fixing some performance bottlenecks from [@deedy5](https://github.com/deedy5) (PR #183)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Workaround potential bug in cpython with Zero Width No-Break Space located in Arabic Presentation Forms-B, Unicode 1.1 not acknowledged as space (PR #175)</span>
<span class="s0">- CLI default threshold aligned with the API threshold from [@oleksandr-kuzmenko](https://github.com/oleksandr-kuzmenko) (PR #181)</span>

<span class="s0">### Removed</span>
<span class="s0">- Support for Python 3.5 (PR #192)</span>

<span class="s0">### Deprecated</span>
<span class="s0">- Use of backport unicodedata from `unicodedata2` as Python is quickly catching up, scheduled for removal in 3.0 (PR #194)</span>

<span class="s0">## [2.0.12](https://github.com/Ousret/charset_normalizer/compare/2.0.11...2.0.12) (2022-02-12)</span>

<span class="s0">### Fixed</span>
<span class="s0">- ASCII miss-detection on rare cases (PR #170) </span>

<span class="s0">## [2.0.11](https://github.com/Ousret/charset_normalizer/compare/2.0.10...2.0.11) (2022-01-30)</span>

<span class="s0">### Added</span>
<span class="s0">- Explicit support for Python 3.11 (PR #164)</span>

<span class="s0">### Changed</span>
<span class="s0">- The logging behavior have been completely reviewed, now using only TRACE and DEBUG levels (PR #163 #165)</span>

<span class="s0">## [2.0.10](https://github.com/Ousret/charset_normalizer/compare/2.0.9...2.0.10) (2022-01-04)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Fallback match entries might lead to UnicodeDecodeError for large bytes sequence (PR #154)</span>

<span class="s0">### Changed</span>
<span class="s0">- Skipping the language-detection (CD) on ASCII (PR #155)</span>

<span class="s0">## [2.0.9](https://github.com/Ousret/charset_normalizer/compare/2.0.8...2.0.9) (2021-12-03)</span>

<span class="s0">### Changed</span>
<span class="s0">- Moderating the logging impact (since 2.0.8) for specific environments (PR #147)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Wrong logging level applied when setting kwarg `explain` to True (PR #146)</span>

<span class="s0">## [2.0.8](https://github.com/Ousret/charset_normalizer/compare/2.0.7...2.0.8) (2021-11-24)</span>
<span class="s0">### Changed</span>
<span class="s0">- Improvement over Vietnamese detection (PR #126)</span>
<span class="s0">- MD improvement on trailing data and long foreign (non-pure latin) data (PR #124)</span>
<span class="s0">- Efficiency improvements in cd/alphabet_languages from [@adbar](https://github.com/adbar) (PR #122)</span>
<span class="s0">- call sum() without an intermediary list following PEP 289 recommendations from [@adbar](https://github.com/adbar) (PR #129)</span>
<span class="s0">- Code style as refactored by Sourcery-AI (PR #131) </span>
<span class="s0">- Minor adjustment on the MD around european words (PR #133)</span>
<span class="s0">- Remove and replace SRTs from assets / tests (PR #139)</span>
<span class="s0">- Initialize the library logger with a `NullHandler` by default from [@nmaynes](https://github.com/nmaynes) (PR #135)</span>
<span class="s0">- Setting kwarg `explain` to True will add provisionally (bounded to function lifespan) a specific stream handler (PR #135)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Fix large (misleading) sequence giving UnicodeDecodeError (PR #137)</span>
<span class="s0">- Avoid using too insignificant chunk (PR #137)</span>

<span class="s0">### Added</span>
<span class="s0">- Add and expose function `set_logging_handler` to configure a specific StreamHandler from [@nmaynes](https://github.com/nmaynes) (PR #135)</span>
<span class="s0">- Add `CHANGELOG.md` entries, format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) (PR #141)</span>

<span class="s0">## [2.0.7](https://github.com/Ousret/charset_normalizer/compare/2.0.6...2.0.7) (2021-10-11)</span>
<span class="s0">### Added</span>
<span class="s0">- Add support for Kazakh (Cyrillic) language detection (PR #109)</span>

<span class="s0">### Changed</span>
<span class="s0">- Further, improve inferring the language from a given single-byte code page (PR #112)</span>
<span class="s0">- Vainly trying to leverage PEP263 when PEP3120 is not supported (PR #116)</span>
<span class="s0">- Refactoring for potential performance improvements in loops from [@adbar](https://github.com/adbar) (PR #113)</span>
<span class="s0">- Various detection improvement (MD+CD) (PR #117)</span>

<span class="s0">### Removed</span>
<span class="s0">- Remove redundant logging entry about detected language(s) (PR #115)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Fix a minor inconsistency between Python 3.5 and other versions regarding language detection (PR #117 #102)</span>

<span class="s0">## [2.0.6](https://github.com/Ousret/charset_normalizer/compare/2.0.5...2.0.6) (2021-09-18)</span>
<span class="s0">### Fixed</span>
<span class="s0">- Unforeseen regression with the loss of the backward-compatibility with some older minor of Python 3.5.x (PR #100)</span>
<span class="s0">- Fix CLI crash when using --minimal output in certain cases (PR #103)</span>

<span class="s0">### Changed</span>
<span class="s0">- Minor improvement to the detection efficiency (less than 1%) (PR #106 #101)</span>

<span class="s0">## [2.0.5](https://github.com/Ousret/charset_normalizer/compare/2.0.4...2.0.5) (2021-09-14)</span>
<span class="s0">### Changed</span>
<span class="s0">- The project now comply with: flake8, mypy, isort and black to ensure a better overall quality (PR #81)</span>
<span class="s0">- The BC-support with v1.x was improved, the old staticmethods are restored (PR #82)</span>
<span class="s0">- The Unicode detection is slightly improved (PR #93)</span>
<span class="s0">- Add syntax sugar \_\_bool\_\_ for results CharsetMatches list-container (PR #91)</span>

<span class="s0">### Removed</span>
<span class="s0">- The project no longer raise warning on tiny content given for detection, will be simply logged as warning instead (PR #92)</span>

<span class="s0">### Fixed</span>
<span class="s0">- In some rare case, the chunks extractor could cut in the middle of a multi-byte character and could mislead the mess detection (PR #95)</span>
<span class="s0">- Some rare 'space' characters could trip up the UnprintablePlugin/Mess detection (PR #96)</span>
<span class="s0">- The MANIFEST.in was not exhaustive (PR #78)</span>

<span class="s0">## [2.0.4](https://github.com/Ousret/charset_normalizer/compare/2.0.3...2.0.4) (2021-07-30)</span>
<span class="s0">### Fixed</span>
<span class="s0">- The CLI no longer raise an unexpected exception when no encoding has been found (PR #70)</span>
<span class="s0">- Fix accessing the 'alphabets' property when the payload contains surrogate characters (PR #68)</span>
<span class="s0">- The logger could mislead (explain=True) on detected languages and the impact of one MBCS match (PR #72)</span>
<span class="s0">- Submatch factoring could be wrong in rare edge cases (PR #72)</span>
<span class="s0">- Multiple files given to the CLI were ignored when publishing results to STDOUT. (After the first path) (PR #72)</span>
<span class="s0">- Fix line endings from CRLF to LF for certain project files (PR #67)</span>

<span class="s0">### Changed</span>
<span class="s0">- Adjust the MD to lower the sensitivity, thus improving the global detection reliability (PR #69 #76)</span>
<span class="s0">- Allow fallback on specified encoding if any (PR #71)</span>

<span class="s0">## [2.0.3](https://github.com/Ousret/charset_normalizer/compare/2.0.2...2.0.3) (2021-07-16)</span>
<span class="s0">### Changed</span>
<span class="s0">- Part of the detection mechanism has been improved to be less sensitive, resulting in more accurate detection results. Especially ASCII. (PR #63)</span>
<span class="s0">- According to the community wishes, the detection will fall back on ASCII or UTF-8 in a last-resort case. (PR #64)</span>

<span class="s0">## [2.0.2](https://github.com/Ousret/charset_normalizer/compare/2.0.1...2.0.2) (2021-07-15)</span>
<span class="s0">### Fixed</span>
<span class="s0">- Empty/Too small JSON payload miss-detection fixed. Report from [@tseaver](https://github.com/tseaver) (PR #59) </span>

<span class="s0">### Changed</span>
<span class="s0">- Don't inject unicodedata2 into sys.modules from [@akx](https://github.com/akx) (PR #57)</span>

<span class="s0">## [2.0.1](https://github.com/Ousret/charset_normalizer/compare/2.0.0...2.0.1) (2021-07-13)</span>
<span class="s0">### Fixed</span>
<span class="s0">- Make it work where there isn't a filesystem available, dropping assets frequencies.json. Report from [@sethmlarson](https://github.com/sethmlarson). (PR #55)</span>
<span class="s0">- Using explain=False permanently disable the verbose output in the current runtime (PR #47)</span>
<span class="s0">- One log entry (language target preemptive) was not show in logs when using explain=True (PR #47)</span>
<span class="s0">- Fix undesired exception (ValueError) on getitem of instance CharsetMatches (PR #52)</span>

<span class="s0">### Changed</span>
<span class="s0">- Public function normalize default args values were not aligned with from_bytes (PR #53)</span>

<span class="s0">### Added</span>
<span class="s0">- You may now use charset aliases in cp_isolation and cp_exclusion arguments (PR #47)</span>

<span class="s0">## [2.0.0](https://github.com/Ousret/charset_normalizer/compare/1.4.1...2.0.0) (2021-07-02)</span>
<span class="s0">### Changed</span>
<span class="s0">- 4x to 5 times faster than the previous 1.4.0 release. At least 2x faster than Chardet.</span>
<span class="s0">- Accent has been made on UTF-8 detection, should perform rather instantaneous.</span>
<span class="s0">- The backward compatibility with Chardet has been greatly improved. The legacy detect function returns an identical charset name whenever possible.</span>
<span class="s0">- The detection mechanism has been slightly improved, now Turkish content is detected correctly (most of the time)</span>
<span class="s0">- The program has been rewritten to ease the readability and maintainability. (+Using static typing)+</span>
<span class="s0">- utf_7 detection has been reinstated.</span>

<span class="s0">### Removed</span>
<span class="s0">- This package no longer require anything when used with Python 3.5 (Dropped cached_property)</span>
<span class="s0">- Removed support for these languages: Catalan, Esperanto, Kazakh, Baque, Volap√ºk, Azeri, Galician, Nynorsk, Macedonian, and Serbocroatian.</span>
<span class="s0">- The exception hook on UnicodeDecodeError has been removed.</span>

<span class="s0">### Deprecated</span>
<span class="s0">- Methods coherence_non_latin, w_counter, chaos_secondary_pass of the class CharsetMatch are now deprecated and scheduled for removal in v3.0</span>

<span class="s0">### Fixed</span>
<span class="s0">- The CLI output used the relative path of the file(s). Should be absolute.</span>

<span class="s0">## [1.4.1](https://github.com/Ousret/charset_normalizer/compare/1.4.0...1.4.1) (2021-05-28)</span>
<span class="s0">### Fixed</span>
<span class="s0">- Logger configuration/usage no longer conflict with others (PR #44)</span>

<span class="s0">## [1.4.0](https://github.com/Ousret/charset_normalizer/compare/1.3.9...1.4.0) (2021-05-21)</span>
<span class="s0">### Removed</span>
<span class="s0">- Using standard logging instead of using the package loguru.</span>
<span class="s0">- Dropping nose test framework in favor of the maintained pytest.</span>
<span class="s0">- Choose to not use dragonmapper package to help with gibberish Chinese/CJK text.</span>
<span class="s0">- Require cached_property only for Python 3.5 due to constraint. Dropping for every other interpreter version.</span>
<span class="s0">- Stop support for UTF-7 that does not contain a SIG.</span>
<span class="s0">- Dropping PrettyTable, replaced with pure JSON output in CLI.</span>

<span class="s0">### Fixed</span>
<span class="s0">- BOM marker in a CharsetNormalizerMatch instance could be False in rare cases even if obviously present. Due to the sub-match factoring process.</span>
<span class="s0">- Not searching properly for the BOM when trying utf32/16 parent codec.</span>

<span class="s0">### Changed</span>
<span class="s0">- Improving the package final size by compressing frequencies.json.</span>
<span class="s0">- Huge improvement over the larges payload.</span>

<span class="s0">### Added</span>
<span class="s0">- CLI now produces JSON consumable output.</span>
<span class="s0">- Return ASCII if given sequences fit. Given reasonable confidence.</span>

<span class="s0">## [1.3.9](https://github.com/Ousret/charset_normalizer/compare/1.3.8...1.3.9) (2021-05-13)</span>

<span class="s0">### Fixed</span>
<span class="s0">- In some very rare cases, you may end up getting encode/decode errors due to a bad bytes payload (PR #40)</span>

<span class="s0">## [1.3.8](https://github.com/Ousret/charset_normalizer/compare/1.3.7...1.3.8) (2021-05-12)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Empty given payload for detection may cause an exception if trying to access the `alphabets` property. (PR #39)</span>

<span class="s0">## [1.3.7](https://github.com/Ousret/charset_normalizer/compare/1.3.6...1.3.7) (2021-05-12)</span>

<span class="s0">### Fixed</span>
<span class="s0">- The legacy detect function should return UTF-8-SIG if sig is present in the payload. (PR #38)</span>

<span class="s0">## [1.3.6](https://github.com/Ousret/charset_normalizer/compare/1.3.5...1.3.6) (2021-02-09)</span>

<span class="s0">### Changed</span>
<span class="s0">- Amend the previous release to allow prettytable 2.0 (PR #35)</span>

<span class="s0">## [1.3.5](https://github.com/Ousret/charset_normalizer/compare/1.3.4...1.3.5) (2021-02-08)</span>

<span class="s0">### Fixed</span>
<span class="s0">- Fix error while using the package with a python pre-release interpreter (PR #33)</span>

<span class="s0">### Changed</span>
<span class="s0">- Dependencies refactoring, constraints revised.</span>

<span class="s0">### Added</span>
<span class="s0">- Add python 3.9 and 3.10 to the supported interpreters</span>

<span class="s0">MIT License</span>

<span class="s0">Copyright (c) 2019 TAHRI Ahmed R.</span>

<span class="s0">Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="s0">of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="s0">in the Software without restriction, including without limitation the rights</span>
<span class="s0">to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="s0">copies of the Software, and to permit persons to whom the Software is</span>
<span class="s0">furnished to do so, subject to the following conditions:</span>

<span class="s0">The above copyright notice and this permission notice shall be included in all</span>
<span class="s0">copies or substantial portions of the Software.</span>

<span class="s0">THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="s0">IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="s0">FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="s0">AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="s0">LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="s0">OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="s0">SOFTWARE.</span>
</pre>
</body>
</html>